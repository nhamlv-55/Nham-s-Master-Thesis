% \NL{spc: smt.total vs }
%  spacer:
%  :time.spacer.solve                                   877.50
%  :time.pool_solver.smt.total                          829.39
% exp: line 5 and line 6 should be more or less the same
% dpy:
%  :time.spacer.solve.reach.gen.bool_ind.outside_spacer 239.81

%  :time.pool_solver.smt.total                          543.69
%  spc :time.spacer.solve                                   810.61

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.3\textwidth]{LaTeX/figures/metric-illustration.pdf}
%   \caption{Running time distribution of \dpy.}
%   \label{fig:metric}
% \end{figure}


\section{Empirical Evaluation}
\label{sec:dopey-exp}

\subsection{Setup.} 
We collect benchmarks from \chccomp\cite{CHC-COMP-18} with a particular focus on benchmarks 
%from the Linear Real Arithmetics track  
that corresponds to verification of hybrid systems, which are known to be challenging for inductive generalization, and for which \spc behaves poorly.
We train \dpy on execution traces of 17 benchmarks (or hybrid systems) and test \dpy on other 170 verification tasks (i.e., 10 different properties for each benchmark).
% , which use different safe properties for the same set of hybrid systems.

% For each benchmark, we create 10 variants of it by changing every non-zero constant $v$ in the $\Bad$ property into a uniformly picked value in the range $[0.95 \times v, 1.05 \times v]$. There are 69 such benchmarks, in which \spc was able to solve 37 of them in our time limit of 15 minutes. 
% Among those 37 benchmarks, 20 of them has at least one entry in $\Ppos$ and $\Pneg$ 
% We focus our evaluation on those benchmarks: the intuition is because \dpy is trained using trace generated by \spc, if \spc failed to solve a benchmark, learning a predictor from its trace will not help solving it.
\noindent
\subsubsection{Training details.} 
We train our model using Adam optimizer \cite{adam}, with embedding dimension of 16 and TreeLSTM' hidden size of 64, dropout rate 0.5, negative sampling rate 5, $\mathit{threshold}$ set to 0.8 for both $\Ppos$ and $\Pneg$. Each model is trained up to 3,000 epochs. All experiments are done using an Ubuntu 20.04 PC with an Nvidia 1050 Ti. 

% \noindent 
% \textbf{Evaluation Metrics.}
% We evaluate \dpy and \spc on a) solving time - time spent on solving an instance b) inductive generalization time - time spent on inductive generalization to solve an instance c) SMT-solving time - time spent on solving SMT-queries to solve an instance d)Inferencing time - time \dpy spends on communicating with \spc, parsing the data, and running model inference.
% Note that by randomly mutating the properties, some can become too hard for \spc and \dpy: in total \spc times out on 43 variants, while \dpy times out on 42 variants, 32 of them are timeout by both.

\subsection{Comparing \dpy with \spc.} 
\dpy successfully solved 41 instances on which \spc failed.\footnote{According to
  \chccomp, failure means no result is produced in 15 minutes.} 
On instances solved by both, the speedups achieved by \dpy are summarized in 
\cref{tab:dpy_vs_spc}. The first column lists the running time of different phases of \dpy, of which the distribution is illustrated in \cref{subfig:flame}. The ``All'', ``Unsafe'', ``Safe'' columns show the average speedups for all instances, instances with unsafe properties (i.e. a counterexample is found), instances with safe properties (i.e. a proof is found), respectively. 
The second row shows that \dpy is 1.51$\times$ faster than \spc on average over all instances solved by both.
The last three rows show the average speedups in specific phases.
% \cref{fig:dpy_vs_spc} and \cref{tab:dpy_vs_spc} show the running time comparision of \dpy vs. \spc. Measuring wallclock time, \dpy is 1.5 times faster than \spc on average, and as shown in \cref{subfig:dpy_vs_spc_total}, is extremely effective on large benchmarks, where the overhead can be offset by gains in skip checking useless queries and try to drop multiple literals together.

% Furthermore, if we exclude inferencing time, which potentially could be shorten with more engineering effort, then \dpy is almost always faster, and on average more than twice as fast compare to \spc, as shown in \cref{subfig:dpy_vs_np_total_sub} and \cref{tab:dpy_vs_spc}. This give us an idea about the upperbound of what \dpy can achieve.

% We also observe a big speedup in inductive generalization: on average, \dpy is 1.58 times faster than \spc for inductive generalization, and 3.26 times faster if we exclude the inferencing time.

Beyond quantitative improvements, \dpy also achieved higher quality of inductive generalization, indicated by the safe instances (Safe column in \cref{tab:dpy_vs_spc}). 
Suppose all the performance gain is from the inductive generalization alone, then the speedups should \emph{decrease} when the overall solving time or SMT solving time is considered (as in the Unsafe column of \cref{tab:dpy_vs_spc}). Surprisingly, this is not the case. 
Our analysis shows that this is because \dpy learns \emph{better} lemmas and improves the reasoning steps \emph{outside} of the inductive generalization phase (SMT-solving time)! %This is welcome but unexpected.
%
    
%Beyond quantative improvements, \dpy also achieved higher quality of inductive generalization, which is indicated by instances with safe properties (shown in the ``safe'' column).
%Suppose the performance gain only comes from inductive generalization, the speedups should \textit{decrease} when we gradually zoom out from the generalization phase, however, this is surprisingly not the case. 
%Our analysis shows that this is because \dpy actually learns better lemmas and improves the reasoning steps \textit{outside} of the inductive generalization phase, which is beyond our expectation.

% However, one might expect to see a similar pattern in \cref{subfig:dpy_vs_spc_total_sub} and \cref{subfig:dpy_vs_spc_ind_gen_sub}, as well as \cref{subfig:dpy_vs_spc_total} and \cref{subfig:dpy_vs_spc_ind_gen}: if other components are not affected, we can expect all the performance gain in solving time can only come from gain in inductive generalization. Yet, somewhat surprisingly, we observe that \dpy generalizes not just faster, but also to \emph{better} lemmas! This can be observed by its effect on other SMT-queries in the system: \cref{subfig:dpy_vs_spc_smt} shows that \dpy always spent less time in SMT-solving, which closely correlate with solving time in \cref{subfig:dpy_vs_spc_total_sub}.

\cref{fig:dpy_vs_spc} plots the running time of each individual instance by \dpy and \spc. Except for a few instances whose solving time is short, \dpy significantly outperforms \spc.

\subsection{Ablation study.}
To highlight the combined benefits of $\Mpos$ and $\Mneg$, we also evaluate \dpy with a \textit{single} model. As shown in \cref{fig:dpy_vs_np}, except for a few outliers, using both models is faster and solves more instances -- \dpy times out on 54 (58) instances when using only $\Mpos$ ($\Mneg$).
% We also evaluate the effect of using $\Mpos$ and $\Mneg$ in isolation. As shown in Figure \ref{fig:dpy_vs_np}, while $\Mpos$ and $\Mneg$ by itself could be faster than \dpy, ultimately they also time out more often: $\Mpos$ times out 54 times, while $\Mneg$ times out 58 times. Out intuition is that $\Mpos$ by itself keeps too many redundant literals, making the generalized lemmas not strong enough, thus makes the whole solving diverge. On the other hand, $\Mneg$ by itself doesn't have many chances to contribute: in our benchmarks, $\Mneg$ suggests a candidate at least once in only 48\% of the benchmarks, meaning that most of the time $\Mneg$ by itself is exactly like \textsc{IterDrop}, but with all the overhead of \dpy.
% \NL{Bug in plotting M+ and M-. M- should look very similar to Spc.}


% Sketch:

% \begin{itemize}
%     \item our implementation
%     \item Datasets and baselines
%     \item Evaluation questions
%     \begin{itemize}
%         \item Do we learn a signal? Does the signal transfer? Does learning that signal helps \spc.
%         \item key result: \tool accelerates \spc on solving \textit{similar} instances
%         \item performance analysis with profling stats (which parts get improved, which parts introduce extra overhead)
%         \item transferability?
%         \item ablation study, e.g. different representations, negative/positive/combined models
%         \item highlight some examples
%     \end{itemize}
%     % \item have some fancy diagrams
% \end{itemize}

% \subsection{Implementation details}
% We implement, train, and run our neural network using PyTorch \cite{pytorch} and communicate with the main solver through gRPC. For each lemma, we only send the lemma formula to the oracle once, and parse, precompute and cache $P\_mat$ and $N\_mat$. For every subsequent call to $query\_model$, we only send over $kept\_lits$ and $2b\_checked\_lits$, which are 2 arrays of indices.

% \subsection{Dataset}

% We evaluate \tool on X LRA benchmark queries from CHC-Comp18 \cite{chccomp18}. CHC-Comp is an annual competition where state-of-the-art solvers compete to solve not just hard and practical problems in the industry, but also handcrafted problems those are specifically design to expose the weaknesses of the current solvers. 

% For each query Q in the benchmark, we use \spc to solve it within a time limit of 15 minutes, and record the inductive generalization trace. We then using this trace to train a model specific to Q, and use it as an oracle for 5 randomly mutated variants of Q. We compare solving time of \tool and \spc on the original query and those 5 variants, using 2 metrics: 1) total solving time, and 2) total solving time inside the solver only (time spent on communication, parsing, and inferencing are not counted). While we do not take training time into the comparison, we report here that training one model takes about 2 hours on a 1050Ti.



% We first train \tool using the data generated by solving those queries, and use \tool as an oracle in solving randomly modified variants of the original queries. We then compare the results against \spc\, the state-of-the-art CHC solver. Since \spc\ is written purely in C++, whereas \tool runs the prediction in a Python process, communicate with the main solver through gRPC, we compare them using both the \textbf{total} solving time and \textbf{inside-solver} solving time (any time spent outside the solver, including communication time, parsing time, and inferencing time, is not included).

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "0.0_main"
%%% End:
