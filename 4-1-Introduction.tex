\section{Introduction}
\label{sec:intro}

The rapid increase of software productivity in the last decades was fueled by the
extensive use of abstraction and reuse of software components. While this
enabled building larger and more complex software systems, these gains came at
the expense of less efficient and less secure software systems, and contribute
to a troublesome trade-off between productivity and performance/security. When
an application built using general-purpose components is deployed, the majority
of the general-purpose functionality is never used. This creates two problems:
first, as the use of abstraction layers makes it more difficult to optimize the
software, this has a detrimental impact on performance; second, it increases the
attack surface for security vulnerabilities.

% Ashish: While OCCAM, Trimmer, (and Chisel, via tests that the delta-debugged version must pass) specialize the application to its deployment context, other debloaters, such as Piece-wise, do not.
An emerging solution to this problem is a set of tools, called \emph{Software
  Debloaters}, that automatically customize a program to a user-specified
environment. One successful approach for software debloating is based on partial
evaluation (PE)~\cite{pe-book} in which a \emph{partial evaluator} takes a
program and some of its input values and produces a \emph{residual}
(or \emph{specialized}) program in which those inputs are replaced with their
values.
%
% AG: not sure why soundness is important at this point of intro
% A sound partial evaluator must ensure that running the residual program on the
% remaining inputs always produces the same result as running the original
% program on all of its inputs.
%
While PE has been extensively applied to functional and logic programs, it was
less successful on imperative C/\cpp programs (with a noteable exception of
\cmix~\cite{Andersen94} and \tempo~\cite{Consel98}). With the advent of
\llvm~\cite{llvm}, several new partial evaluators of LLVM bitcode have arisen
during the last few years (e.g., \llpe~\cite{llpe}, \occam~\cite{occam}, and
\trimmer~\cite{trimmer}). These tools leverage LLVM optimizations such as
constant propagation, function inlining, and others to either reduce the size of
the residual program (e.g., \occam and \trimmer) or improve performance (e.g.,
\llpe).

The key step of a PE-based debloater is to estimate whether specializing (or
inlining) a function will result in a smaller (or more secure) residual program.
%Unfortunately, as with most problems in program analysis, this is very difficult.
Specialization naturally increases the size of the code since it adds extra
functions. However, since some inputs in the new functions have been replaced by
constant values, optimizations such as constant propagation, might become
enabled and result in new optimization opportunities that reduce the code size.
Therefore, the decision whether or not a function should be specialized for a
particular call-site is non-trivial. % where the solution must be found
% in a large search space that represents the effects of other specialized call
% sites and all LLVM optimizations applied after those specializations.
Current PE-based software debloaters use naive heuristics. For example, \occam
implements two heuristics of ``never specialize'' or ``always specialize'',
respectively; \trimmer specializes a function only if it is called once in the
whole program. 

In this paper, we present a new approach based on reinforcement
learning (RL) to automatically infer effective heuristics for
specializing functions for PE-based software debloating. RL is a good
fit for the problem for two reasons. First, code specialization
resembles a Markov Decision Process: each decision moves the code from
one state to another, and specialization depends only on the current
state rather than the history of the transformations.  Second, the
quality of debloating can only be measured after all specialization
actions and corresponding compiler optimizations have been
performed. % Rendering approaches like
% supervised-learning inapplicable.

The main challenge in applying RL is in deciding on a good state
representation.  While it is tempting to use the source code (or LLVM
intermediate representation (IR)) as a state, this is not
computationally tractable. The IR is typically hundreds of MBs in
size. Instead, an adequate set of features that captures meaningful
information about the code while avoiding state aliasing is
required. In particular, these features must capture the \emph{calling context}
in order to distinguish between call-sites.

The main contributions of this paper are threefold: (1) calling
context features that enable RL to find a useful heuristic for
PE-based debloating software; (2) implementation of our method in a
tool called \doccam; and (3) evaluation on the reduction of the
program size and number of possible code-reuse attacks, by comparing
our handcrafted features with features learnt automatically via
embedding LLVM IR into a vector space using \insttovec~\cite{inst2vec}.
%
Our initial evaluation suggests that RL is a viable method for
developing an effective specialization policy for debloating, and learning with
handcrafted features is easier than with \insttovec.


%% The main contribution of this paper is a \emph{calling context}
%% feature that enables RL to find a useful heuristic for debloating software.
%% \nl{not sure where to put this and how to phrase it properly: We compare our
%%   handcrafted set of features with features learnt automatically via embedding
%%   LLVM IR into vector space using \insttovec~\cite{inst2vec}, by implementing
%%   both of them in \occam~\cite{occam}, a state-of-the-art
%% PE-based software debloater for LLVM bitcode}
%% \sout{We
%% have implemented our new method in \occam~\cite{occam}, a state-of-the-art
%% PE-based software debloater for LLVM bitcode}. We evaluate our tool, called
%% \doccam, on the reduction of program size and the number of possible code-reuse
%% attacks~\cite{gsa}. Our evaluation suggests that RL is a viable method for
%% developing an effective specialization policy for debloating. \nl{struggle with
%%   how to end intro} 
%on \emph{online partial evaluation} (e.g., ).

%% \emph{the debloating problem}

%% Developing large and complex softwares are easier than ever thanks to open
%% sourced frameworks and libraries, providing many needed building blocks to
%% developers so that they do not have to spend time reinventing the wheel.
%% However, this convenience comes with a cost: to be able to serve as many use
%% cases as possible, libraries and frameworks are often loaded with
%% too many functionalities that no single client could possible use all of them.
%% And the clients themselves are not free of this problem either: cURL
%% \cite{curl}, a popular tool to transfer data from or to a server, supports 23
%% different protocols, many of them are outdated. Those bloated softwares lead to
%% many problems such as size, performance, and security. In this paper, we will
%%   focus on how to debloat the software for the security purposes.

%% \emph{the specialization solution}

%% Due to the severe problems caused by bloated software, debloating is a well
%% studied problems. Different tools deploy different technique to remove bloats:
%% CHISEL \cite{chisel} combines reinforcement learning with delta-debugging to
%% make use of test suites to skim the code; TRIMMER \cite{trimmer} assumes that
%% many running arguments are known beforehand and propagate that information
%% during compile time; compilers themselves, such as LLVM \cite{llvm}, are also
%% optimized to favor smaller binaries, and can be set to prioritize size
%% above all else.
%% \nl{Not sure if we should mention the following paragraph}
%% Each has its own pros and cons: CHISEL achieves the highest
%% reduction rate but is not ``sound'' because the correctness of the resulted program is only
%% as good as its test suites, TRIMMER is sound but its performance is weaker, and
%% relying on compiler for code debloating leads to weak results but doesn't
%% require any extra information from programmers, hence it is the most convenient
%% solution.

%% In our work, we focus on producing ``sound'' and debloated softwares, hence our
%% main comparision is with TRIMMER and LLVM, but we include here results from
%% CHISEL as well, for reference.
%% \nl{I can compile CHISEL but due to the very different natures of the two
%%   approaches, I am not sure what is the best way to do comparision. CHISEL
%%   will keep skimming the code as long as the final binaries pass the same tests as the
%%   original binaries, while TRIMMER and OCCAM require \emph{manifests}} 
%% TRIMMER relies on a special code transformation called ``specialization'', in
%% which at a call site, instead of invoking the original callee, we create a
%% special version of the callee, and replace all known arguments (deducted from the
%% provided manifest) with constants, and inline the callee. Doing this
%% transformation naively, the code base will actually be more bloated, since too
%% many callsites are replaced by inlined functions. TRIMMER manages that problem by
%% using a simple memory model, combined with loop unrolling and a pessimistic heuristic in which only
%% callees those are known to be called exactly once are inlined.
%% \emph{hook to explain why we are experimenting reinforcement learning in this
%%   setting}

%% In this paper, we present an approach using reinforcement learning to simplify
%% the process of designing heuristics for a broader range of metrics. Our
%% heuristic acts on the same action space as TRIMMER, namely whether to
%% \emph{specialize} a function or not, and our early results show that we can
%% achieve compatable results without using an explicit memory models and
%% handcrafted heuristics.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "neurips_2019"
%%% End:
